sl_tokenization_pre_text = """

Preparing data for training a Large Language Model (LLM) involves several key steps:

1. **📚 Identify the corpus of data**: Large and diverse set of text data that the model will learn from.
2. **🔍 Break down the data into tokens**: Tokenize the identified data into smaller units (tokens) that the model can process.
3. **📖 Generate dictionary**: Create a dictionary of unique tokens, sorted and ranked, to be used by the model during training.
"""


sl_tokenization_post_text = """
In the next section, we will learn BytePairEncoding (BPE) tokenization. This is the most common tokenization technique and was used to train the models such as GPT-3 released by OpenAI.
"""